includes:
- config/jarvis/qmof/bandgap/random/base.yml

model:
 name: graph_attention_transformer
 irreps_node_embedding: '128x0e+64x0o+64x1e+64x1o'
 num_layers: 4
 irreps_node_attr: '1x0e'
 use_node_attr: False
 irreps_sh: '1x0e+1x1o'
 max_radius: 6.0
 max_neighbors: 50
 number_of_basis: 128
 fc_neurons: [64, 64]
 use_atom_edge_attr: False
 irreps_atom_edge_attr: '1x0e'
 irreps_feature: '128x0e'
 irreps_head: '32x0e+8x0o+8x1e+8x1o'
 num_heads: 8
 irreps_pre_attn: '128x0e+64x0o+64x1e+64x1o'
 rescale_degree: False
 nonlinear_message: True
 irreps_mlp_mid: '256x0e+192x0o+192x1e+192x1o'
 norm_layer: 'layer'
 alpha_drop: 0.2
 proj_drop: 0.0
 out_drop: 0.0
 drop_path_rate: 0.0
 otf_graph: False
 use_pbc: True

optim:
 batch_size: 2
 eval_batch_size: 2
 num_workers: 4
 lr_initial: 0.0002
 optimizer: AdamW
 optimizer_params: 
  weight_decay: 0.001
 scheduler: LambdaLR
 scheduler_params:
  lambda_type: cosine
  warmup_factor: 0.2
  warmup_epochs: 20
  epochs: 500
  lr_min_factor: 1.e-2
 max_epochs: 10
 eval_every: 500
